{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Analysis\n",
    "\n",
    "In this notebook we explore how glove embeddings can be used to create a noisy set for training. In particular given a set of `BIO` encoded sequences, extract the positive labels, and use them to do some analysis in the embedding space/augment the set of positive labels.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "glove_embeddings <- load_glove()\n",
    "\n",
    "gather positive labeled words (both word and phrase level)\n",
    "\n",
    "augment set of positive labeled words\n",
    "    - kNN with FAISS\n",
    "    - logistic regression / SVM kernels for hyperplane\n",
    "    - linear transforms/Affine Transforms\n",
    "\n",
    "Analyze augmented set\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- FAISS (Facebook AI Similarity Search) for kNN style search with\n",
    "    - L2 distance\n",
    "    - Cosine Simlarity\n",
    "- Scikit Learn: Logistic Regression\n",
    "- Scikit Learn: SVM Kernels\n",
    "- PyTorch: Linear Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Callable,\n",
    "    Optional,\n",
    ")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import allennlp\n",
    "\n",
    "# local imports\n",
    "import dpd\n",
    "from dpd.dataset.bio_dataset import BIODataset\n",
    "from dpd.utils import (\n",
    "    remove_bio,\n",
    "    explain_labels,\n",
    "    get_words,\n",
    ")\n",
    "\n",
    "from dpd.constants import (\n",
    "    CONLL2003_TRAIN,\n",
    "    CONLL2003_VALID,\n",
    "    CADEC_TRAIN,\n",
    "    CADEC_VALID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generic constants to help throughout\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "GLOVE_EMBEDDING_DIR = 'data/glove.6B'\n",
    "GLOVE_DIMS = [50, 100, 200, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings\n",
    "In this section we look to load the GLOVE word embeddings.\n",
    "\n",
    "Input: `file_path: str`\n",
    "\n",
    "Output: `Dict[str, np.ndarray]` word to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful TypeDefs\n",
    "EmbeddingType = np.ndarray\n",
    "EmbeddingSpaceType = Dict[str, EmbeddingType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dim_file(dims: int, include_base=True) -> str:\n",
    "    '''\n",
    "    Given a number of dimensions, return the associated GLOVE embedding file\n",
    "    \n",
    "    Input: ``dims`` int, the number of dims\n",
    "            ``include_base``, should include the full file path or just the file name\n",
    "    Output: ``file_name`` str, the name of the associated file\n",
    "    \n",
    "    raises: Exception if number of dims is not available\n",
    "    '''\n",
    "    if dims not in GLOVE_DIMS:\n",
    "        raise Exception(f'Unknown dims: {dims} only have {GLOVE_DIMS}')\n",
    "    \n",
    "    glove_file = f'glove.6B.{dims}d.txt'\n",
    "    if include_base:\n",
    "        glove_file = os.path.join(GLOVE_EMBEDDING_DIR, glove_file)\n",
    "    return glove_file\n",
    "\n",
    "def load_glove(dims: int) -> EmbeddingSpaceType:\n",
    "    '''\n",
    "    Given a number of dimensions load the embedding space for the associated GLOVE embedding\n",
    "    \n",
    "    Input: ``dims``: int the number of dimensions to use\n",
    "    \n",
    "    Output: ``EmbeddingSpace`` EmbeddingSpaceType, the entire embedding space embedded in the file\n",
    "    '''\n",
    "    glove_file = get_glove_dim_file(dims, include_base=True)\n",
    "    with open(glove_file, 'r') as f:\n",
    "        embedding_space = {}\n",
    "        for line in tqdm(f):\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            embedding_space[word] = embedding\n",
    "    return embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:44, 9078.14it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = load_glove(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will use our dataset readers that we implemented and load the CADEC dataset\n",
    "In particular we will look at the ADR tag, however this should be generalized, so\n",
    "we implement a set of functions to load our data given\n",
    "\n",
    "- DataSet Type (CADEC, CONLL)\n",
    "- Dataset Class (e.g. `ADR`, `PER`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96867it [00:00, 343559.76it/s]\n",
      "24143it [00:00, 360596.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_files(dataset_type: str) -> Tuple[str, str]:\n",
    "    if dataset_type == 'CONLL':\n",
    "        return CONLL2003_TRAIN, CONLL2003_VALID\n",
    "    elif dataset_type == 'CADEC':\n",
    "        return CADEC_TRAIN, CADEC_VALID\n",
    "    else:\n",
    "        raise Exception(f'Unknown dataset: {dataset_type}')\n",
    "\n",
    "def load_data(\n",
    "    dataset_type: str,\n",
    "    binary_class: Optional[str] = None,\n",
    ") -> Tuple[BIODataset, BIODataset]:\n",
    "    '''\n",
    "    Load BIODataset for a given dataset type with the binary\n",
    "    class if specified\n",
    "    '''\n",
    "    train_file, valid_file = get_dataset_files(dataset_type)\n",
    "\n",
    "    train_dataset = BIODataset(\n",
    "        dataset_id=0,\n",
    "        file_name=train_file,\n",
    "        binary_class=binary_class,\n",
    "    )\n",
    "    \n",
    "    train_dataset.parse_file()\n",
    "    \n",
    "    valid_dataset = BIODataset(\n",
    "        dataset_id=1,\n",
    "        file_name=valid_file,\n",
    "        binary_class=binary_class, \n",
    "    )\n",
    "    \n",
    "    valid_dataset.parse_file()\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "train_data, valid_data = load_data('CADEC', 'ADR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process BIO Data\n",
    "\n",
    "Now that we have loaded all the proper data, we need to process the dataset, and in particular create two dictionaries\n",
    "\n",
    "1. Word level\n",
    "2. Phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_random_sample(data: BIODataset, sample_size: int) -> BIODataset:\n",
    "    '''\n",
    "    Genereate a random sample of the data\n",
    "    '''\n",
    "    bio_data = list(data.data)\n",
    "    bio_dataset = BIODataset(\n",
    "        dataset_id=2,\n",
    "        file_name='temp.txt',\n",
    "        binary_class=data.binary_class,\n",
    "    )\n",
    "\n",
    "    def _random_sample_array(array: List[object], size: int):\n",
    "        for i in range(len(array)):\n",
    "            ind = random.randint(0, len(array) - 1)\n",
    "            temp = array[i]\n",
    "            array[i] = array[ind]\n",
    "            array[ind] = temp\n",
    "\n",
    "        return array[:size]\n",
    "    \n",
    "    bio_dataset.data = bio_data[:sample_size] #_random_sample_array(bio_data, sample_size)\n",
    "    \n",
    "    return bio_dataset\n",
    "train_data_sample = bio_random_sample(train_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter: Dict[str, Counter] = {'pos': Counter(), 'neg': Counter()}\n",
    "phrase_counter: Counter = Counter()\n",
    "for entry in train_data_sample:\n",
    "    sentence, tags = entry['input'], entry['output']\n",
    "    pos_words = get_words(sentence, tags, train_data_sample.binary_class)\n",
    "    \n",
    "    # get the negative words\n",
    "    neg_words = get_words(sentence, tags, 'O')\n",
    "\n",
    "    pos_ranges, pos_phrases = explain_labels(sentence, tags)\n",
    "    for w in pos_words:\n",
    "        word_counter['pos'][w] += 1\n",
    "\n",
    "    for w in neg_words:\n",
    "        word_counter['neg'][w] += 1\n",
    "\n",
    "    for phrase in pos_phrases:\n",
    "        phrase_counter[tuple(phrase)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now we have everything setup\n",
    "\n",
    "- `glove_embeddings`: this contains a map of word to numpy array (`np.ndarray`) of the glove embedding\n",
    "- `train_data_sample`: a random sample of the training data\n",
    "- `word_counter`: pos word to count and neg word to count\n",
    "- `phrase_counter`: count of tuples of the most popular entities\n",
    "\n",
    "Now we can use this data to determine how to best augment our dictionaries and define functions seperating them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "First lets look at how we can augment this dictionary through the embedding space with kNN.\n",
    "\n",
    "- `L2 Distance` find the K nearest neighbors through the L2 distance\n",
    "- `Cosine Similarity` find the K nearest neighbors through the cosine similarity\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Build Index\n",
    "    - build a `FAISS` index\n",
    "    - map word to index in glove\n",
    "    - map index to word in glove\n",
    "2. Search Index\n",
    "    - build a `np.ndarray` query\n",
    "3. Experiment with Hyper Parameters\n",
    "    - different algorithms listed above\n",
    "    - experiment with various values of k\n",
    "\n",
    "#### Hypothesis\n",
    "\n",
    "While visualizing the embedding space an help, our thought is that there may be different hyper planes that are closer than we anticipate, something like logisitc regression, SVM, or affine transforms might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_snorkel)",
   "language": "python",
   "name": "conda_snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
