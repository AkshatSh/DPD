{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Analysis\n",
    "\n",
    "In this notebook we explore how glove embeddings can be used to create a noisy set for training. In particular given a set of `BIO` encoded sequences, extract the positive labels, and use them to do some analysis in the embedding space/augment the set of positive labels.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "glove_embeddings <- load_glove()\n",
    "\n",
    "gather positive labeled words (both word and phrase level)\n",
    "\n",
    "augment set of positive labeled words\n",
    "    - kNN with FAISS\n",
    "    - logistic regression / SVM kernels for hyperplane\n",
    "    - linear transforms/Affine Transforms\n",
    "\n",
    "Analyze augmented set\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- FAISS (Facebook AI Similarity Search) for kNN style search with\n",
    "    - L2 distance\n",
    "    - Cosine Simlarity\n",
    "- Scikit Learn: Logistic Regression\n",
    "- Scikit Learn: SVM Kernels\n",
    "- PyTorch: Linear Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Callable,\n",
    "    Optional,\n",
    ")\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import allennlp\n",
    "\n",
    "# local imports\n",
    "import dpd\n",
    "from dpd.dataset.bio_dataset import BIODataset\n",
    "from dpd.utils import (\n",
    "    remove_bio,\n",
    "    explain_labels,\n",
    "    get_words,\n",
    ")\n",
    "\n",
    "from dpd.constants import (\n",
    "    CONLL2003_TRAIN,\n",
    "    CONLL2003_VALID,\n",
    "    CADEC_TRAIN,\n",
    "    CADEC_VALID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generic constants to help throughout\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "GLOVE_EMBEDDING_DIR = 'data/glove.6B'\n",
    "GLOVE_DIMS = [50, 100, 200, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings\n",
    "In this section we look to load the GLOVE word embeddings.\n",
    "\n",
    "Input: `file_path: str`\n",
    "\n",
    "Output: `Dict[str, np.ndarray]` word to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful TypeDefs\n",
    "EmbeddingType = np.ndarray\n",
    "EmbeddingSpaceType = Dict[str, EmbeddingType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dim_file(dims: int, include_base=True) -> str:\n",
    "    '''\n",
    "    Given a number of dimensions, return the associated GLOVE embedding file\n",
    "    \n",
    "    Input: ``dims`` int, the number of dims\n",
    "            ``include_base``, should include the full file path or just the file name\n",
    "    Output: ``file_name`` str, the name of the associated file\n",
    "    \n",
    "    raises: Exception if number of dims is not available\n",
    "    '''\n",
    "    if dims not in GLOVE_DIMS:\n",
    "        raise Exception(f'Unknown dims: {dims} only have {GLOVE_DIMS}')\n",
    "    \n",
    "    glove_file = f'glove.6B.{dims}d.txt'\n",
    "    if include_base:\n",
    "        glove_file = os.path.join(GLOVE_EMBEDDING_DIR, glove_file)\n",
    "    return glove_file\n",
    "\n",
    "def load_glove(dims: int) -> EmbeddingSpaceType:\n",
    "    '''\n",
    "    Given a number of dimensions load the embedding space for the associated GLOVE embedding\n",
    "    \n",
    "    Input: ``dims``: int the number of dimensions to use\n",
    "    \n",
    "    Output: ``EmbeddingSpace`` EmbeddingSpaceType, the entire embedding space embedded in the file\n",
    "    '''\n",
    "    glove_file = get_glove_dim_file(dims, include_base=True)\n",
    "    with open(glove_file, 'r') as f:\n",
    "        embedding_space = {}\n",
    "        for line in tqdm(f):\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            embedding_space[word] = embedding\n",
    "    return embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:44, 8928.75it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_dim = 300\n",
    "glove_embeddings = load_glove(glove_embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will use our dataset readers that we implemented and load the CADEC dataset\n",
    "In particular we will look at the ADR tag, however this should be generalized, so\n",
    "we implement a set of functions to load our data given\n",
    "\n",
    "- DataSet Type (CADEC, CONLL)\n",
    "- Dataset Class (e.g. `ADR`, `PER`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96867it [00:00, 343559.76it/s]\n",
      "24143it [00:00, 360596.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_files(dataset_type: str) -> Tuple[str, str]:\n",
    "    if dataset_type == 'CONLL':\n",
    "        return CONLL2003_TRAIN, CONLL2003_VALID\n",
    "    elif dataset_type == 'CADEC':\n",
    "        return CADEC_TRAIN, CADEC_VALID\n",
    "    else:\n",
    "        raise Exception(f'Unknown dataset: {dataset_type}')\n",
    "\n",
    "def load_data(\n",
    "    dataset_type: str,\n",
    "    binary_class: Optional[str] = None,\n",
    ") -> Tuple[BIODataset, BIODataset]:\n",
    "    '''\n",
    "    Load BIODataset for a given dataset type with the binary\n",
    "    class if specified\n",
    "    '''\n",
    "    train_file, valid_file = get_dataset_files(dataset_type)\n",
    "\n",
    "    train_dataset = BIODataset(\n",
    "        dataset_id=0,\n",
    "        file_name=train_file,\n",
    "        binary_class=binary_class,\n",
    "    )\n",
    "    \n",
    "    train_dataset.parse_file()\n",
    "    \n",
    "    valid_dataset = BIODataset(\n",
    "        dataset_id=1,\n",
    "        file_name=valid_file,\n",
    "        binary_class=binary_class, \n",
    "    )\n",
    "    \n",
    "    valid_dataset.parse_file()\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "train_data, valid_data = load_data('CADEC', 'ADR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process BIO Data\n",
    "\n",
    "Now that we have loaded all the proper data, we need to process the dataset, and in particular create two dictionaries\n",
    "\n",
    "1. Word level\n",
    "2. Phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_random_sample(data: BIODataset, sample_size: int) -> BIODataset:\n",
    "    '''\n",
    "    Genereate a random sample of the data\n",
    "    '''\n",
    "    bio_data = list(data.data)\n",
    "    bio_dataset = BIODataset(\n",
    "        dataset_id=2,\n",
    "        file_name='temp.txt',\n",
    "        binary_class=data.binary_class,\n",
    "    )\n",
    "\n",
    "    def _random_sample_array(array: List[object], size: int):\n",
    "        for i in range(len(array)):\n",
    "            ind = random.randint(0, len(array) - 1)\n",
    "            temp = array[i]\n",
    "            array[i] = array[ind]\n",
    "            array[ind] = temp\n",
    "\n",
    "        return array[:size]\n",
    "    \n",
    "    bio_dataset.data = bio_data[:sample_size] #_random_sample_array(bio_data, sample_size)\n",
    "    \n",
    "    return bio_dataset\n",
    "train_data_sample = bio_random_sample(train_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter: Dict[str, Counter] = {'pos': Counter(), 'neg': Counter()}\n",
    "phrase_counter: Counter = Counter()\n",
    "for entry in train_data_sample:\n",
    "    sentence, tags = entry['input'], entry['output']\n",
    "    pos_words = get_words(sentence, tags, train_data_sample.binary_class)\n",
    "    \n",
    "    # get the negative words\n",
    "    neg_words = get_words(sentence, tags, 'O')\n",
    "\n",
    "    pos_ranges, pos_phrases = explain_labels(sentence, tags)\n",
    "    for w in pos_words:\n",
    "        word_counter['pos'][w] += 1\n",
    "\n",
    "    for w in neg_words:\n",
    "        word_counter['neg'][w] += 1\n",
    "\n",
    "    for phrase in pos_phrases:\n",
    "        phrase_counter[tuple(phrase)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now we have everything setup\n",
    "\n",
    "- `glove_embeddings`: this contains a map of word to numpy array (`np.ndarray`) of the glove embedding\n",
    "- `train_data_sample`: a random sample of the training data\n",
    "- `word_counter`: pos word to count and neg word to count\n",
    "- `phrase_counter`: count of tuples of the most popular entities\n",
    "\n",
    "Now we can use this data to determine how to best augment our dictionaries and define functions seperating them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "First lets look at how we can augment this dictionary through the embedding space with kNN.\n",
    "\n",
    "- `L2 Distance` find the K nearest neighbors through the L2 distance\n",
    "- `Cosine Similarity` find the K nearest neighbors through the cosine similarity\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Build Index\n",
    "    - build a `FAISS` index\n",
    "    - map word to index in glove\n",
    "    - map index to word in glove\n",
    "2. Search Index\n",
    "    - build a `np.ndarray` query\n",
    "3. Experiment with Hyper Parameters\n",
    "    - different algorithms listed above\n",
    "    - experiment with various values of k\n",
    "\n",
    "#### Hypothesis\n",
    "\n",
    "While visualizing the embedding space an help, our thought is that there may be different hyper planes that are closer than we anticipate, something like logisitc regression, SVM, or affine transforms might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityAlgorithm(Enum):\n",
    "    L2Distance = 1\n",
    "    CosineSimilarity = 2\n",
    "\n",
    "class WordEmbeddingIndex(object):\n",
    "    '''\n",
    "    Build a FAISS index for an EmbeddingSpaceType\n",
    "    object\n",
    "    \n",
    "    Its a nice wrapper around the faiss index, to allow easily searching\n",
    "    and converting vectors to words and vice versa\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_space: EmbeddingSpaceType,\n",
    "        embedding_space_dims: int,\n",
    "        similarity_algorithm: SimilarityAlgorithm,\n",
    "    ):\n",
    "        self.embedding_space = embedding_space\n",
    "        self.embedding_space_dims = embedding_space_dims\n",
    "        self.similarity_algorithm = SimilarityAlgorithm\n",
    "        self.index_np, self.word_to_index, self.index_to_word = (\n",
    "            WordEmbeddingIndex.build_index(\n",
    "                embedding_space,\n",
    "                embedding_space_dims,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # for FAISS we need float32 instead of float64\n",
    "        self.index_np = self.index_np.astype('float32')\n",
    "        \n",
    "        self.faiss_index = faiss.IndexFlatIP(embedding_space_dims)\n",
    "        if similarity_algorithm == SimilarityAlgorithm.CosineSimilarity:\n",
    "            # normalize with L2 as a proxy for cosine search\n",
    "            faiss.normalize_L2(self.index_np)\n",
    "        self.faiss_index.add(self.index_np)\n",
    "    \n",
    "    def find_similar(\n",
    "        self,\n",
    "        query_np: np.ndarray,\n",
    "        k: int,\n",
    "        remove_first_row: bool = True,\n",
    "    ) -> Counter:\n",
    "        '''\n",
    "        given a query retreive similar words\n",
    "        \n",
    "        input:\n",
    "            - ``query_np`` np.ndarray\n",
    "                The query to search the embedding space for\n",
    "        output:\n",
    "            - ``Counter``\n",
    "                The count of kNN results\n",
    "        '''\n",
    "        query_np = query_np.astype('float32')\n",
    "        distances, indexes = self.faiss_index.search(query_np, k)\n",
    "        \n",
    "        if remove_first_row:\n",
    "            first_row = indexes[:, 0]\n",
    "#             assert (self.index_np[embedding_indicies[0]] - self.index_np[first_row[0]]).sum() == 0\n",
    "#             assert (query_np[0] - self.index_np[first_row[0]]).sum() == 0\n",
    "            similar_words_i = indexes[:, 0:]\n",
    "        else:\n",
    "            similar_words_i = indexes\n",
    "        \n",
    "        similar_words_i = similar_words_i.flatten()\n",
    "        \n",
    "        similar_words = Counter()\n",
    "        for word_index in similar_words_i:\n",
    "            similar_words[self.index_to_word[word_index]] += 1\n",
    "        return similar_words\n",
    "    \n",
    "    def _phrase_embedding(\n",
    "        self,\n",
    "        phrase: List[str],\n",
    "    ) -> np.ndarray:\n",
    "        '''\n",
    "        compute an embedding representation given a list of words\n",
    "        input:\n",
    "            - ``phrase`` List[str]\n",
    "                \n",
    "        output:\n",
    "            - ``np.ndarray``\n",
    "                the embedding for the phrase\n",
    "        '''\n",
    "        phrase_embedding = np.zeros((self.embedding_space_dims,))\n",
    "        for w in phrase:\n",
    "            embedding_index = self.get_embedding_index(w)\n",
    "            if w not in STOP_WORDS and embedding_index > 0:\n",
    "                continue\n",
    "            embedding_vec = self.index_np[embedding_index]\n",
    "            phrase_embedding += embedding_vec\n",
    "        phrase_embedding /= len(phrase)\n",
    "        return phrase_embedding\n",
    "    \n",
    "    def find_similar_phrases(\n",
    "        self,\n",
    "        query: List[List[str]],\n",
    "        k: int = 5,\n",
    "    ) -> Counter:\n",
    "        query_vecs = [self._phrase_embedding(q) for q in query]\n",
    "        query_np = np.array(query_vecs)\n",
    "        similar_words = self.find_similar(query_np, k, remove_first_row=False)\n",
    "        for phrase in query:\n",
    "            for word in phrase:\n",
    "                del similar_words[word]\n",
    "        return similar_words\n",
    "\n",
    "    def get_embedding(\n",
    "        self,\n",
    "        word: str,\n",
    "    ) -> np.ndarray:\n",
    "        '''\n",
    "        Retrieving the embedding for a specific word\n",
    "        '''\n",
    "        embedding_i = self.get_embedding_index(word)\n",
    "        return self.index_np[embedding_i]\n",
    "\n",
    "    def find_similar_words(\n",
    "        self,\n",
    "        query: List[str],\n",
    "        k: int = 5,\n",
    "    ) -> Counter:\n",
    "        '''\n",
    "        Using the specified search algorithm and the query passed in, the method\n",
    "        returns similar words\n",
    "        \n",
    "        The algorithm builds a query of embedding vectors by retrieving the cached embedding vectors\n",
    "        Then uses the `similarity search` specified in the constructor to find similary queries\n",
    "        Finally the indexes are converted to words and a list of query words is retrieved and ranked\n",
    "        by ocurrence.\n",
    "        \n",
    "        input:\n",
    "            - ``query``: List[str]\n",
    "                a list of all the query words, this should be the dictionary (or a subset) that\n",
    "                we are augmenting\n",
    "            - ``k``: int,\n",
    "                the number of instances to search over (the k in kNN)\n",
    "            - ``result_size``: Optional[int]\n",
    "                if specified will limit the results to be of the result size\n",
    "        output:\n",
    "            - ``similar words`` Counter[str]\n",
    "                a counting occurence of all the words retrieved from the query\n",
    "        '''\n",
    "        embedding_indicies_list = list(set([self.get_embedding_index(w) for w in query]))\n",
    "        embedding_indicies = list(filter(lambda x: x > 0, embedding_indicies_list))\n",
    "        embedding_indicies = np.array(embedding_indicies)\n",
    "        \n",
    "        query_np = self.index_np[embedding_indicies]\n",
    "        \n",
    "        similar_words = self.find_similar(query_np, k)\n",
    "        for word in query:\n",
    "            del similar_words[word]\n",
    "        return similar_words\n",
    "    \n",
    "    def get_embedding_index(self, word: str) -> np.ndarray:\n",
    "        if word not in self.word_to_index:\n",
    "            word = 'UNK'\n",
    "        return self.word_to_index[word]   \n",
    "    \n",
    "    @classmethod\n",
    "    def build_index(\n",
    "        cls,\n",
    "        embedding_space: EmbeddingSpaceType,\n",
    "        embedding_space_dims: int,\n",
    "    ) -> Tuple[np.ndarray, Dict[str, int], Dict[int, str]]:\n",
    "        '''\n",
    "        Builds 3 objects specified in the output, meant for searching in the\n",
    "        embedding space\n",
    "        \n",
    "        input:\n",
    "            - ``embedding_space``: EmbeddingSpaceType\n",
    "                this is the embedding space mapping keys to embeddings, we use this\n",
    "                to create a nice wrapper around FAISS to enable fast searching\n",
    "            - ``embedding_space_dims``: int\n",
    "                the number of dimensions in each embedding\n",
    "        output Tuple of 3 object:\n",
    "            - ``index_np`` np.ndarray\n",
    "                shape: (len(embedding_space), embedding space dimensions)\n",
    "                this contains the entire index of the embedding space in a continous numpy\n",
    "                ndarray for searching\n",
    "            - ``word_to_index`` Dict[str, int]\n",
    "                maps each word to the associated index in the index_np\n",
    "            - ``index_to_word`` Dict[int, str]\n",
    "                maps each index to the associated word\n",
    "        '''\n",
    "        word_to_index = {'UNK': 0}\n",
    "        index_to_word = {0: 'UNK'}\n",
    "        for word in embedding_space:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            index_to_word[\n",
    "                word_to_index[word]\n",
    "            ] = word\n",
    "        \n",
    "        index_np = np.ndarray((len(word_to_index), embedding_space_dims))\n",
    "\n",
    "        # first dimension is UNK\n",
    "        index_np[0] = np.zeros((embedding_space_dims,))\n",
    "        for word, embedding in embedding_space.items():\n",
    "            word_i = word_to_index[word]\n",
    "            index_np[word_i] = embedding\n",
    "        \n",
    "        return index_np, word_to_index, index_to_word\n",
    "\n",
    "word_embedding_index = WordEmbeddingIndex(\n",
    "    glove_embeddings,\n",
    "    glove_embedding_dim,\n",
    "    SimilarityAlgorithm.CosineSimilarity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding space search: word_embedding_index\n",
    "# query: word_counter['pos'] (len: 293)\n",
    "# results come from: word_embedding_index.find_similar_words(word_counter['pos'], k=5, result_size=10)\n",
    "similar_words = word_embedding_index.find_similar_words(\n",
    "    list(word_counter['pos'].keys()),\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "similar_phrases = word_embedding_index.find_similar_phrases(\n",
    "    list(phrase_counter.keys()),\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ankle', 7),\n",
       " ('vomiting', 6),\n",
       " ('shortness', 6),\n",
       " ('groin', 4),\n",
       " ('anxiety', 4),\n",
       " ('hamstring', 3),\n",
       " ('elbow', 3),\n",
       " ('maybe', 3),\n",
       " ('worried', 3),\n",
       " ('lips', 3),\n",
       " ('losing', 3),\n",
       " ('wrist', 3),\n",
       " ('hand', 3),\n",
       " ('migraines', 3),\n",
       " ('itching', 3),\n",
       " ('you', 3),\n",
       " ('but', 2),\n",
       " ('dark', 2),\n",
       " ('blue', 2),\n",
       " ('increase', 2)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Results\n",
    "\n",
    "The results above show that the embedding space search is bring in relevant items, however it seems like some of these items are not relevant, which could be the issue related to the hyperplanes mentioned earlier.\n",
    "\n",
    "To counter act this next we take a look at logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Here we employ the algorithm to learn a logistic regression classifier.\n",
    "\n",
    "1. Create a train set with all positive words and an equal amount of negative words (exclude stop words)\n",
    "2. Fit a logisitc regression model\n",
    "3. Use this to classifiy every point in the embedding space as a part of the dictionary or not\n",
    "\n",
    "The hope here is that the logistic regression model can form a hyperplane that seperate concepts better than the kNN model could, since nearest neighbors could be in some random dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_embeddings = []\n",
    "train_set_labels = []\n",
    "import string\n",
    "\n",
    "num_pos = 0\n",
    "for w in word_counter['pos'].keys():\n",
    "    if w in STOP_WORDS or w in string.punctuation:\n",
    "        continue\n",
    "    embedding_vec = word_embedding_index.get_embedding(w)\n",
    "    train_set_labels.append(1)\n",
    "    train_set_embeddings.append(embedding_vec)\n",
    "    num_pos += 1\n",
    "\n",
    "num_neg = 0\n",
    "for w in word_counter['neg'].keys():\n",
    "    if w in STOP_WORDS or w in string.punctuation:\n",
    "        continue\n",
    "    embedding_vec = word_embedding_index.get_embedding(w)\n",
    "    train_set_labels.append(0)\n",
    "    train_set_embeddings.append(embedding_vec)\n",
    "    num_neg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/snorkel/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_train = np.array(train_set_embeddings)\n",
    "y_train = np.array(train_set_labels)\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = logisticRegr.predict(word_embedding_index.index_np)\n",
    "probs = logisticRegr.predict_proba(word_embedding_index.index_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs.shape (index_size, 2)\n",
    "similar_words_lr = {'pos': Counter(), 'neg': Counter()}\n",
    "for i, (label, prob) in enumerate(zip(labels, probs)):\n",
    "    key = 'pos' if label == 1 else 'neg'\n",
    "    counter = similar_words_lr[key]\n",
    "    word = word_embedding_index.index_to_word[i]\n",
    "    if word in STOP_WORDS or word in string.punctuation or w in word_counter['pos']:\n",
    "        continue\n",
    "    counter[word] += prob[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numbness', 0.9174100396266176),\n",
       " ('rashes', 0.9012706273738842),\n",
       " ('dizziness', 0.8934524597467283),\n",
       " ('protruding', 0.890337491119129),\n",
       " ('bruised', 0.8853579871679472),\n",
       " ('irritability', 0.8801254161047158),\n",
       " ('itchy', 0.879740547126885),\n",
       " ('swollen', 0.8773934862144145),\n",
       " ('lethargy', 0.8740313251993611),\n",
       " ('blisters', 0.8736541852951585)]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words_lr['pos'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs\n",
    "\n",
    "Lets try the same thing with SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear', probability=True) \n",
    "svclassifier.fit(x_train, y_train)\n",
    "labels = svclassifier.predict(word_embedding_index.index_np)\n",
    "probs = svclassifier.predict_proba(word_embedding_index.index_np)\n",
    "similar_words_svm = {'pos': Counter(), 'neg': Counter()}\n",
    "for i, (label, prob) in enumerate(zip(labels, probs)):\n",
    "    key = 'pos' if label == 1 else 'neg'\n",
    "    counter = similar_words_svm[key]\n",
    "    word = word_embedding_index.index_to_word[i]\n",
    "    if word in STOP_WORDS or word in string.punctuation or w in word_counter['pos']:\n",
    "        continue\n",
    "    counter[word] += prob[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numbness', 0.9374348638457773),\n",
       " ('dizziness', 0.9318870428755427),\n",
       " ('rashes', 0.9279577857644434),\n",
       " ('blisters', 0.9260811004453761),\n",
       " ('faint', 0.919302595417729),\n",
       " ('itching', 0.9160872487326459),\n",
       " ('bruised', 0.9144254605848144),\n",
       " ('tingling', 0.9092695025666933),\n",
       " ('slurred', 0.9092653569745792),\n",
       " ('coughing', 0.9091544259898877)]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words_svm['pos'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM RBF?\n",
    "Lets try the same thing with an `rbf` kernel instead of a `linear` kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/snorkel/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf', probability=True) \n",
    "svclassifier.fit(x_train, y_train)\n",
    "labels = svclassifier.predict(word_embedding_index.index_np)\n",
    "probs = svclassifier.predict_proba(word_embedding_index.index_np)\n",
    "similar_words_svm_rbf = {'pos': Counter(), 'neg': Counter()}\n",
    "for i, (label, prob) in enumerate(zip(labels, probs)):\n",
    "    if prob[1] > prob[0]:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    key = 'pos' if label == 1 else 'neg'\n",
    "    counter = similar_words_svm_rbf[key]\n",
    "    word = word_embedding_index.index_to_word[i]\n",
    "    if word in STOP_WORDS or word in string.punctuation or w in word_counter['pos']:\n",
    "        continue\n",
    "    counter[word] += prob[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numbness', 0.9754717961932726),\n",
       " ('dizziness', 0.9703606725879994),\n",
       " ('rashes', 0.9639200074280232),\n",
       " ('blisters', 0.9593204541186061),\n",
       " ('nausea', 0.9588400021115162),\n",
       " ('cramps', 0.9584717111222454),\n",
       " ('cramping', 0.9572098637354005),\n",
       " ('twitching', 0.9568297853283206),\n",
       " ('vomiting', 0.9529760285921908),\n",
       " ('aches', 0.9519459599576058)]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words_svm_rbf['pos'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Polynomial Degree 2?\n",
    "\n",
    "You know the drill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/snorkel/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='poly', degree=2, probability=True)\n",
    "svclassifier.fit(x_train, y_train)\n",
    "labels = svclassifier.predict(word_embedding_index.index_np)\n",
    "probs = svclassifier.predict_proba(word_embedding_index.index_np)\n",
    "similar_words_svm_ply = {'pos': Counter(), 'neg': Counter()}\n",
    "for i, (label, prob) in enumerate(zip(labels, probs)):\n",
    "    if prob[1] > prob[0]:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    key = 'pos' if label == 1 else 'neg'\n",
    "    counter = similar_words_svm_ply[key]\n",
    "    word = word_embedding_index.index_to_word[i]\n",
    "    if word in STOP_WORDS or word in string.punctuation or w in word_counter['pos']:\n",
    "        continue\n",
    "    counter[word] += prob[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dizziness', 0.9729899342296005),\n",
       " ('nausea', 0.9691237668776509),\n",
       " ('numbness', 0.9625200586567948),\n",
       " ('headaches', 0.9544747070153324),\n",
       " ('vomiting', 0.9526546886960093),\n",
       " ('cramps', 0.9509906356922635),\n",
       " ('aches', 0.9445665677362626),\n",
       " ('cramping', 0.9305996447244286),\n",
       " ('sore', 0.9267912627285241),\n",
       " ('aching', 0.9263559710717603)]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words_svm_ply['pos'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "In this section lets take a look at an error analysis of the validation set.\n",
    "\n",
    "Sine we are looking at dictionary models, lets use a dictionary based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_sample = bio_random_sample(valid_data, 100)\n",
    "valid_word_counter: Dict[str, Counter] = {'pos': Counter(), 'neg': Counter()}\n",
    "valid_phrase_counter: Counter = Counter()\n",
    "for entry in valid_data_sample:\n",
    "    sentence, tags = entry['input'], entry['output']\n",
    "    pos_words = get_words(sentence, tags, train_data_sample.binary_class)\n",
    "    \n",
    "    # get the negative words\n",
    "    neg_words = get_words(sentence, tags, 'O')\n",
    "\n",
    "    pos_ranges, pos_phrases = explain_labels(sentence, tags)\n",
    "    for w in pos_words:\n",
    "        valid_word_counter['pos'][w] += 1\n",
    "\n",
    "    for w in neg_words:\n",
    "        valid_word_counter['neg'][w] += 1\n",
    "\n",
    "    for phrase in pos_phrases:\n",
    "        valid_phrase_counter[tuple(phrase)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_counter = {'pos': Counter(), 'neg': Counter()}\n",
    "def analyze_dict(known_dict: Counter, aug_dict: Counter, valid_dict: Dict[str, Counter]) -> Tuple[float, float, float]:\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    not_found = 0\n",
    "    \n",
    "    pred_dict = {\n",
    "        'pos': known_dict['pos'] + aug_dict['pos'],\n",
    "        'neg': known_dict['neg'] + aug_dict['neg'],\n",
    "    }\n",
    "\n",
    "    # pos_dict = [w for (w, _) in pred_dict['pos'].most_common(N)]\n",
    "    \n",
    "    for valid_w in valid_dict['pos']:\n",
    "        if valid_w in STOP_WORDS:\n",
    "            continue\n",
    "        if valid_w in pred_dict['pos']:\n",
    "            correct += 1\n",
    "        elif valid_w in pred_dict['neg']:\n",
    "            incorrect += 1\n",
    "        else:\n",
    "            not_found += 1\n",
    "    \n",
    "    return (correct, incorrect, not_found, correct / (correct + incorrect + not_found))\n",
    "\n",
    "def compute_f1(dict_model: Counter, valid_data) -> Tuple[float, float, float]:\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    def _pred(dict_model: Counter, word: str, pred_class: str) -> str:\n",
    "        if word in dict_model:\n",
    "            return pred_class\n",
    "        else:\n",
    "            return 'O'\n",
    "    for entry in valid_data_sample:\n",
    "        sentence, tags = entry['input'], entry['output']\n",
    "        for i, (s_i, t_i) in enumerate(zip(sentence, tags)):\n",
    "            t_i = remove_bio(t_i)\n",
    "            p_i = _pred(dict_model, s_i, 'ADR')\n",
    "            \n",
    "            if p_i == 'ADR':\n",
    "                if t_i == p_i:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if t_i == p_i:\n",
    "                    tn += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "    precision = float(tp) / float(tp + fp + 1e-13) # avoid / 0\n",
    "    recall = float(tp) / float(tp + fn + 1e-13) # avoid / 0\n",
    "    f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n",
    "    return (precision, recall, f1_measure)\n",
    "\n",
    "def counter_f1(known_dict: Counter, aug_dict: Counter, valid_data):\n",
    "    def _merge_counter(known_dict, aug_dict, top_k = None):\n",
    "        if top_k is None:\n",
    "            return list((known_dict['pos'] + aug_dict['pos']).keys())\n",
    "        result = list(known_dict.keys())\n",
    "        aug_dict_top_k = [w for w, _ in aug_dict['pos'].most_common(top_k)]\n",
    "        return result + aug_dict_top_k\n",
    "\n",
    "    dict_model = _merge_counter(known_dict, aug_dict, top_k=None)\n",
    "    return compute_f1(dict_model, valid_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply = analyze_dict(word_counter, similar_words_svm_ply, valid_word_counter)\n",
    "norm_dict = analyze_dict(word_counter, empty_counter, valid_word_counter)\n",
    "knn_dict = analyze_dict(word_counter, {'pos': similar_words, 'neg': Counter()}, valid_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 38, 244, 0.41735537190082644)\n",
      "(197, 177, 110, 0.40702479338842973)\n",
      "(137, 56, 291, 0.2830578512396694)\n"
     ]
    }
   ],
   "source": [
    "print(knn_dict)\n",
    "print(ply)\n",
    "print(norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr (0.3632124352331606, 0.5603517186250999, 0.4407419050612537)\n",
      "svm-linear-kernel (0.36512820512820515, 0.5691446842525979, 0.4448609809434076)\n",
      "svm-rbf-kernel (0.359979633401222, 0.5651478816946442, 0.4398133748055512)\n",
      "svm-ply-2-kernel (0.3619246861924686, 0.5531574740207834, 0.4375592791653016)\n",
      "kNN (0.2041994750656168, 0.6219024780175859, 0.3074491207270918)\n",
      "no-aug (0.33520809898762655, 0.47641886490807356, 0.39352921756350384)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "print('lr', counter_f1(word_counter, similar_words_lr, valid_data))\n",
    "\n",
    "# SVM Linear\n",
    "print('svm-linear-kernel', counter_f1(word_counter, similar_words_svm, valid_data))\n",
    "\n",
    "# SVM RBF\n",
    "print('svm-rbf-kernel', counter_f1(word_counter, similar_words_svm_rbf, valid_data))\n",
    "\n",
    "# SVM PLY\n",
    "print('svm-ply-2-kernel', counter_f1(word_counter, similar_words_svm_ply, valid_data))\n",
    "\n",
    "# kNN\n",
    "print('kNN', counter_f1(word_counter, {'pos': similar_words, 'neg': Counter()}, valid_data))\n",
    "\n",
    "# No augmentation\n",
    "print('no-aug', counter_f1(word_counter, empty_counter, valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24085"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_words_lr['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_snorkel)",
   "language": "python",
   "name": "conda_snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
