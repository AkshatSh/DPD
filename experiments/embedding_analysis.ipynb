{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Analysis\n",
    "\n",
    "In this notebook we explore how glove embeddings can be used to create a noisy set for training. In particular given a set of `BIO` encoded sequences, extract the positive labels, and use them to do some analysis in the embedding space/augment the set of positive labels.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "glove_embeddings <- load_glove()\n",
    "\n",
    "gather positive labeled words (both word and phrase level)\n",
    "\n",
    "augment set of positive labeled words\n",
    "    - kNN with FAISS\n",
    "    - logistic regression / SVM kernels for hyperplane\n",
    "    - linear transforms/Affine Transforms\n",
    "\n",
    "Analyze augmented set\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- FAISS (Facebook AI Similarity Search) for kNN style search with\n",
    "    - L2 distance\n",
    "    - Cosine Simlarity\n",
    "- Scikit Learn: Logistic Regression\n",
    "- Scikit Learn: SVM Kernels\n",
    "- PyTorch: Linear Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Callable,\n",
    "    Optional,\n",
    ")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import allennlp\n",
    "\n",
    "# local imports\n",
    "import dpd\n",
    "from dpd.dataset.bio_dataset import BIODataset\n",
    "from dpd.utils import (\n",
    "    remove_bio\n",
    ")\n",
    "\n",
    "from dpd.constants import (\n",
    "    CONLL2003_TRAIN,\n",
    "    CONLL2003_VALID,\n",
    "    CADEC_TRAIN,\n",
    "    CADEC_VALID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generic constants to help throughout\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "GLOVE_EMBEDDING_DIR = 'data/glove.6B'\n",
    "GLOVE_DIMS = [50, 100, 200, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings\n",
    "In this section we look to load the GLOVE word embeddings.\n",
    "\n",
    "Input: `file_path: str`\n",
    "\n",
    "Output: `Dict[str, np.ndarray]` word to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful TypeDefs\n",
    "EmbeddingType = np.ndarray\n",
    "EmbeddingSpaceType = Dict[str, EmbeddingType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dim_file(dims: int, include_base=True) -> str:\n",
    "    '''\n",
    "    Given a number of dimensions, return the associated GLOVE embedding file\n",
    "    \n",
    "    Input: ``dims`` int, the number of dims\n",
    "            ``include_base``, should include the full file path or just the file name\n",
    "    Output: ``file_name`` str, the name of the associated file\n",
    "    \n",
    "    raises: Exception if number of dims is not available\n",
    "    '''\n",
    "    if dims not in GLOVE_DIMS:\n",
    "        raise Exception(f'Unknown dims: {dims} only have {GLOVE_DIMS}')\n",
    "    \n",
    "    glove_file = f'glove.6B.{dims}d.txt'\n",
    "    if include_base:\n",
    "        glove_file = os.path.join(GLOVE_EMBEDDING_DIR, glove_file)\n",
    "    return glove_file\n",
    "\n",
    "def load_glove(dims: int) -> EmbeddingSpaceType:\n",
    "    '''\n",
    "    Given a number of dimensions load the embedding space for the associated GLOVE embedding\n",
    "    \n",
    "    Input: ``dims``: int the number of dimensions to use\n",
    "    \n",
    "    Output: ``EmbeddingSpace`` EmbeddingSpaceType, the entire embedding space embedded in the file\n",
    "    '''\n",
    "    glove_file = get_glove_dim_file(dims, include_base=True)\n",
    "    with open(glove_file, 'r') as f:\n",
    "        embedding_space = {}\n",
    "        for line in tqdm(f):\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            embedding_space[word] = embedding\n",
    "    return embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d027041dd740a78b522b95592810b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = load_glove(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will use our dataset readers that we implemented and load the CADEC dataset\n",
    "In particular we will look at the ADR tag, however this should be generalized, so\n",
    "we implement a set of functions to load our data given\n",
    "\n",
    "- DataSet Type (CADEC, CONLL)\n",
    "- Dataset Class (e.g. `ADR`, `PER`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96867it [00:00, 264653.12it/s]\n",
      "24143it [00:00, 337541.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_files(dataset_type: str) -> Tuple[str, str]:\n",
    "    if dataset_type == 'CONLL':\n",
    "        return CONLL2003_TRAIN, CONLL2003_VALID\n",
    "    elif dataset_type == 'CADEC':\n",
    "        return CADEC_TRAIN, CADEC_VALID\n",
    "    else:\n",
    "        raise Exception(f'Unknown dataset: {dataset_type}')\n",
    "\n",
    "def load_data(\n",
    "    dataset_type: str,\n",
    "    binary_class: Optional[str] = None,\n",
    ") -> Tuple[BIODataset, BIODataset]:\n",
    "    '''\n",
    "    Load BIODataset for a given dataset type with the binary\n",
    "    class if specified\n",
    "    '''\n",
    "    train_file, valid_file = get_dataset_files(dataset_type)\n",
    "\n",
    "    train_dataset = BIODataset(\n",
    "        dataset_id=0,\n",
    "        file_name=train_file,\n",
    "        binary_class=binary_class,\n",
    "    )\n",
    "    \n",
    "    train_dataset.parse_file()\n",
    "    \n",
    "    valid_dataset = BIODataset(\n",
    "        dataset_id=1,\n",
    "        file_name=valid_file,\n",
    "        binary_class=binary_class, \n",
    "    )\n",
    "    \n",
    "    valid_dataset.parse_file()\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "train_data, valid_data = load_data('CADEC', 'ADR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process BIO Data\n",
    "\n",
    "Now that we have loaded all the proper data, we need to process the dataset, and in particular create two dictionaries\n",
    "\n",
    "1. Word level\n",
    "2. Phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence: List[str], tags: List[str]) -> Dict[str, List[str]]:\n",
    "    output = {}\n",
    "    for word, tag in zip(sentence, tags):\n",
    "        if word in STOP_WORDS:\n",
    "            continue\n",
    "        r_tag = remove_bio(tag)\n",
    "        output[r_tag] = word\n",
    "    return output\n",
    "\n",
    "def get_phrase(sentence: List[str], tags: List[str]) -> List[str]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_snorkel)",
   "language": "python",
   "name": "conda_snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
