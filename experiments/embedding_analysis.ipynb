{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Analysis\n",
    "\n",
    "In this notebook we explore how glove embeddings can be used to create a noisy set for training. In particular given a set of `BIO` encoded sequences, extract the positive labels, and use them to do some analysis in the embedding space/augment the set of positive labels.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "```\n",
    "glove_embeddings <- load_glove()\n",
    "\n",
    "gather positive labeled words (both word and phrase level)\n",
    "\n",
    "augment set of positive labeled words\n",
    "    - kNN with FAISS\n",
    "    - logistic regression / SVM kernels for hyperplane\n",
    "    - linear transforms/Affine Transforms\n",
    "\n",
    "Analyze augmented set\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- FAISS (Facebook AI Similarity Search) for kNN style search with\n",
    "    - L2 distance\n",
    "    - Cosine Simlarity\n",
    "- Scikit Learn: Logistic Regression\n",
    "- Scikit Learn: SVM Kernels\n",
    "- PyTorch: Linear Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Callable,\n",
    "    Optional,\n",
    ")\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import allennlp\n",
    "\n",
    "# local imports\n",
    "import dpd\n",
    "from dpd.dataset.bio_dataset import BIODataset\n",
    "from dpd.utils import (\n",
    "    remove_bio,\n",
    "    explain_labels,\n",
    "    get_words,\n",
    ")\n",
    "\n",
    "from dpd.constants import (\n",
    "    CONLL2003_TRAIN,\n",
    "    CONLL2003_VALID,\n",
    "    CADEC_TRAIN,\n",
    "    CADEC_VALID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generic constants to help throughout\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "GLOVE_EMBEDDING_DIR = 'data/glove.6B'\n",
    "GLOVE_DIMS = [50, 100, 200, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Embeddings\n",
    "In this section we look to load the GLOVE word embeddings.\n",
    "\n",
    "Input: `file_path: str`\n",
    "\n",
    "Output: `Dict[str, np.ndarray]` word to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful TypeDefs\n",
    "EmbeddingType = np.ndarray\n",
    "EmbeddingSpaceType = Dict[str, EmbeddingType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dim_file(dims: int, include_base=True) -> str:\n",
    "    '''\n",
    "    Given a number of dimensions, return the associated GLOVE embedding file\n",
    "    \n",
    "    Input: ``dims`` int, the number of dims\n",
    "            ``include_base``, should include the full file path or just the file name\n",
    "    Output: ``file_name`` str, the name of the associated file\n",
    "    \n",
    "    raises: Exception if number of dims is not available\n",
    "    '''\n",
    "    if dims not in GLOVE_DIMS:\n",
    "        raise Exception(f'Unknown dims: {dims} only have {GLOVE_DIMS}')\n",
    "    \n",
    "    glove_file = f'glove.6B.{dims}d.txt'\n",
    "    if include_base:\n",
    "        glove_file = os.path.join(GLOVE_EMBEDDING_DIR, glove_file)\n",
    "    return glove_file\n",
    "\n",
    "def load_glove(dims: int) -> EmbeddingSpaceType:\n",
    "    '''\n",
    "    Given a number of dimensions load the embedding space for the associated GLOVE embedding\n",
    "    \n",
    "    Input: ``dims``: int the number of dimensions to use\n",
    "    \n",
    "    Output: ``EmbeddingSpace`` EmbeddingSpaceType, the entire embedding space embedded in the file\n",
    "    '''\n",
    "    glove_file = get_glove_dim_file(dims, include_base=True)\n",
    "    with open(glove_file, 'r') as f:\n",
    "        embedding_space = {}\n",
    "        for line in tqdm(f):\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            embedding_space[word] = embedding\n",
    "    return embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:44, 8928.75it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_dim = 300\n",
    "glove_embeddings = load_glove(glove_embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will use our dataset readers that we implemented and load the CADEC dataset\n",
    "In particular we will look at the ADR tag, however this should be generalized, so\n",
    "we implement a set of functions to load our data given\n",
    "\n",
    "- DataSet Type (CADEC, CONLL)\n",
    "- Dataset Class (e.g. `ADR`, `PER`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96867it [00:00, 343559.76it/s]\n",
      "24143it [00:00, 360596.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_files(dataset_type: str) -> Tuple[str, str]:\n",
    "    if dataset_type == 'CONLL':\n",
    "        return CONLL2003_TRAIN, CONLL2003_VALID\n",
    "    elif dataset_type == 'CADEC':\n",
    "        return CADEC_TRAIN, CADEC_VALID\n",
    "    else:\n",
    "        raise Exception(f'Unknown dataset: {dataset_type}')\n",
    "\n",
    "def load_data(\n",
    "    dataset_type: str,\n",
    "    binary_class: Optional[str] = None,\n",
    ") -> Tuple[BIODataset, BIODataset]:\n",
    "    '''\n",
    "    Load BIODataset for a given dataset type with the binary\n",
    "    class if specified\n",
    "    '''\n",
    "    train_file, valid_file = get_dataset_files(dataset_type)\n",
    "\n",
    "    train_dataset = BIODataset(\n",
    "        dataset_id=0,\n",
    "        file_name=train_file,\n",
    "        binary_class=binary_class,\n",
    "    )\n",
    "    \n",
    "    train_dataset.parse_file()\n",
    "    \n",
    "    valid_dataset = BIODataset(\n",
    "        dataset_id=1,\n",
    "        file_name=valid_file,\n",
    "        binary_class=binary_class, \n",
    "    )\n",
    "    \n",
    "    valid_dataset.parse_file()\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "train_data, valid_data = load_data('CADEC', 'ADR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process BIO Data\n",
    "\n",
    "Now that we have loaded all the proper data, we need to process the dataset, and in particular create two dictionaries\n",
    "\n",
    "1. Word level\n",
    "2. Phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_random_sample(data: BIODataset, sample_size: int) -> BIODataset:\n",
    "    '''\n",
    "    Genereate a random sample of the data\n",
    "    '''\n",
    "    bio_data = list(data.data)\n",
    "    bio_dataset = BIODataset(\n",
    "        dataset_id=2,\n",
    "        file_name='temp.txt',\n",
    "        binary_class=data.binary_class,\n",
    "    )\n",
    "\n",
    "    def _random_sample_array(array: List[object], size: int):\n",
    "        for i in range(len(array)):\n",
    "            ind = random.randint(0, len(array) - 1)\n",
    "            temp = array[i]\n",
    "            array[i] = array[ind]\n",
    "            array[ind] = temp\n",
    "\n",
    "        return array[:size]\n",
    "    \n",
    "    bio_dataset.data = bio_data[:sample_size] #_random_sample_array(bio_data, sample_size)\n",
    "    \n",
    "    return bio_dataset\n",
    "train_data_sample = bio_random_sample(train_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter: Dict[str, Counter] = {'pos': Counter(), 'neg': Counter()}\n",
    "phrase_counter: Counter = Counter()\n",
    "for entry in train_data_sample:\n",
    "    sentence, tags = entry['input'], entry['output']\n",
    "    pos_words = get_words(sentence, tags, train_data_sample.binary_class)\n",
    "    \n",
    "    # get the negative words\n",
    "    neg_words = get_words(sentence, tags, 'O')\n",
    "\n",
    "    pos_ranges, pos_phrases = explain_labels(sentence, tags)\n",
    "    for w in pos_words:\n",
    "        word_counter['pos'][w] += 1\n",
    "\n",
    "    for w in neg_words:\n",
    "        word_counter['neg'][w] += 1\n",
    "\n",
    "    for phrase in pos_phrases:\n",
    "        phrase_counter[tuple(phrase)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now we have everything setup\n",
    "\n",
    "- `glove_embeddings`: this contains a map of word to numpy array (`np.ndarray`) of the glove embedding\n",
    "- `train_data_sample`: a random sample of the training data\n",
    "- `word_counter`: pos word to count and neg word to count\n",
    "- `phrase_counter`: count of tuples of the most popular entities\n",
    "\n",
    "Now we can use this data to determine how to best augment our dictionaries and define functions seperating them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "First lets look at how we can augment this dictionary through the embedding space with kNN.\n",
    "\n",
    "- `L2 Distance` find the K nearest neighbors through the L2 distance\n",
    "- `Cosine Similarity` find the K nearest neighbors through the cosine similarity\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Build Index\n",
    "    - build a `FAISS` index\n",
    "    - map word to index in glove\n",
    "    - map index to word in glove\n",
    "2. Search Index\n",
    "    - build a `np.ndarray` query\n",
    "3. Experiment with Hyper Parameters\n",
    "    - different algorithms listed above\n",
    "    - experiment with various values of k\n",
    "\n",
    "#### Hypothesis\n",
    "\n",
    "While visualizing the embedding space an help, our thought is that there may be different hyper planes that are closer than we anticipate, something like logisitc regression, SVM, or affine transforms might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityAlgorithm(Enum):\n",
    "    L2Distance = 1\n",
    "    CosineSimilarity = 2\n",
    "\n",
    "class WordEmbeddingIndex(object):\n",
    "    '''\n",
    "    Build a FAISS index for an EmbeddingSpaceType\n",
    "    object\n",
    "    \n",
    "    Its a nice wrapper around the faiss index, to allow easily searching\n",
    "    and converting vectors to words and vice versa\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_space: EmbeddingSpaceType,\n",
    "        embedding_space_dims: int,\n",
    "        similarity_algorithm: SimilarityAlgorithm,\n",
    "    ):\n",
    "        self.embedding_space = embedding_space\n",
    "        self.embedding_space_dims = embedding_space_dims\n",
    "        self.similarity_algorithm = SimilarityAlgorithm\n",
    "        self.index_np, self.word_to_index, self.index_to_word = (\n",
    "            WordEmbeddingIndex.build_index(\n",
    "                embedding_space,\n",
    "                embedding_space_dims,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # for FAISS we need float32 instead of float64\n",
    "        self.index_np = self.index_np.astype('float32')\n",
    "        \n",
    "        self.faiss_index = faiss.IndexFlatIP(embedding_space_dims)\n",
    "        if similarity_algorithm == SimilarityAlgorithm.CosineSimilarity:\n",
    "            # normalize with L2 as a proxy for cosine search\n",
    "            # faiss.normalize_L2(self.index_np)\n",
    "            pass\n",
    "        self.faiss_index.add(self.index_np)\n",
    "    \n",
    "    def find_similar_words(\n",
    "        self,\n",
    "        query: List[str],\n",
    "        k: int = 5,\n",
    "        result_size: Optional[int] = None,\n",
    "    ) -> Counter:\n",
    "        '''\n",
    "        Using the specified search algorithm and the query passed in, the method\n",
    "        returns similar words\n",
    "        \n",
    "        The algorithm builds a query of embedding vectors by retrieving the cached embedding vectors\n",
    "        Then uses the `similarity search` specified in the constructor to find similary queries\n",
    "        Finally the indexes are converted to words and a list of query words is retrieved and ranked\n",
    "        by ocurrence.\n",
    "        \n",
    "        input:\n",
    "            - ``query``: List[str]\n",
    "                a list of all the query words, this should be the dictionary (or a subset) that\n",
    "                we are augmenting\n",
    "            - ``k``: int,\n",
    "                the number of instances to search over (the k in kNN)\n",
    "            - ``result_size``: Optional[int]\n",
    "                if specified will limit the results to be of the result size\n",
    "        output:\n",
    "            - ``similar words`` Counter[str]\n",
    "                a counting occurence of all the words retrieved from the query\n",
    "        '''\n",
    "        SMALL_NUM_QUERIES = 6\n",
    "        embedding_indicies_list = list(set([self.get_embedding_index(w) for w in query]))\n",
    "        embedding_indicies = list(filter(lambda x: x > 0, embedding_indicies_list))\n",
    "        embedding_indicies = np.array(embedding_indicies)\n",
    "        \n",
    "        query_np = self.index_np[embedding_indicies]\n",
    "        \n",
    "        distances, indexes = self.faiss_index.search(query_np, k)\n",
    "        \n",
    "        first_row = indexes[:, 0]\n",
    "        assert (self.index_np[embedding_indicies[0]] - self.index_np[first_row[0]]).sum() == 0\n",
    "        assert (query_np[0] - self.index_np[first_row[0]]).sum() == 0\n",
    "        \n",
    "        similar_words_i = indexes[:, 0:].flatten()\n",
    "        \n",
    "        similar_words = Counter()\n",
    "        for word_index in similar_words_i:\n",
    "            similar_words[self.index_to_word[word_index]] += 1\n",
    "        return similar_words\n",
    "    \n",
    "    def get_embedding_index(self, word: str) -> np.ndarray:\n",
    "        if word not in self.word_to_index:\n",
    "            word = 'UNK'\n",
    "        return self.word_to_index[word]   \n",
    "    \n",
    "    @classmethod\n",
    "    def build_index(\n",
    "        cls,\n",
    "        embedding_space: EmbeddingSpaceType,\n",
    "        embedding_space_dims: int,\n",
    "    ) -> Tuple[np.ndarray, Dict[str, int], Dict[int, str]]:\n",
    "        '''\n",
    "        Builds 3 objects specified in the output, meant for searching in the\n",
    "        embedding space\n",
    "        \n",
    "        input:\n",
    "            - ``embedding_space``: EmbeddingSpaceType\n",
    "                this is the embedding space mapping keys to embeddings, we use this\n",
    "                to create a nice wrapper around FAISS to enable fast searching\n",
    "            - ``embedding_space_dims``: int\n",
    "                the number of dimensions in each embedding\n",
    "        output Tuple of 3 object:\n",
    "            - ``index_np`` np.ndarray\n",
    "                shape: (len(embedding_space), embedding space dimensions)\n",
    "                this contains the entire index of the embedding space in a continous numpy\n",
    "                ndarray for searching\n",
    "            - ``word_to_index`` Dict[str, int]\n",
    "                maps each word to the associated index in the index_np\n",
    "            - ``index_to_word`` Dict[int, str]\n",
    "                maps each index to the associated word\n",
    "        '''\n",
    "        word_to_index = {'UNK': 0}\n",
    "        index_to_word = {0: 'UNK'}\n",
    "        for word in embedding_space:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            index_to_word[\n",
    "                word_to_index[word]\n",
    "            ] = word\n",
    "        \n",
    "        index_np = np.ndarray((len(word_to_index), embedding_space_dims))\n",
    "\n",
    "        # first dimension is UNK\n",
    "        index_np[0] = np.zeros((embedding_space_dims,))\n",
    "        for word, embedding in embedding_space.items():\n",
    "            word_i = word_to_index[word]\n",
    "            index_np[word_i] = embedding\n",
    "        \n",
    "        return index_np, word_to_index, index_to_word\n",
    "\n",
    "word_embedding_index = WordEmbeddingIndex(\n",
    "    glove_embeddings,\n",
    "    glove_embedding_dim,\n",
    "    SimilarityAlgorithm.L2Distance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding space search: word_embedding_index\n",
    "# query: word_counter['pos'] (len: 293)\n",
    "# results come from: word_embedding_index.find_similar_words(word_counter['pos'], k=5, result_size=10)\n",
    "similar_words = word_embedding_index.find_similar_words(\n",
    "    list(word_counter['pos'].keys()),\n",
    "    k=5,\n",
    "    result_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nausea', 23),\n",
       " ('dizziness', 17),\n",
       " ('vomiting', 10),\n",
       " ('aches', 8),\n",
       " ('knee', 8),\n",
       " ('ankle', 8),\n",
       " ('abdominal', 7),\n",
       " ('i', 7),\n",
       " ('diarrhea', 7),\n",
       " ('shortness', 7),\n",
       " ('groin', 5),\n",
       " ('hips', 5),\n",
       " ('knees', 5),\n",
       " ('you', 5),\n",
       " ('tingling', 5),\n",
       " (\"n't\", 5),\n",
       " ('muscles', 4),\n",
       " ('thigh', 4),\n",
       " ('shoulders', 4),\n",
       " ('pain', 4)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_snorkel)",
   "language": "python",
   "name": "conda_snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
