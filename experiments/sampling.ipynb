{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional, Set\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# local library imports\n",
    "import dpd\n",
    "from dpd.models.embedder import CachedTextFieldEmbedder\n",
    "from dpd.constants import CADEC_ELMo, CADEC_BERT\n",
    "from dpd.utils import H5SaveFile, get_all_embedders, TensorList, get_dataset_files\n",
    "from dpd.dataset import BIODatasetReader, BIODataset, UnlabeledBIODataset, ActiveBIODataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our Contextual Word Representors\n",
    "cwr_bert, cwr_elmo = get_all_embedders()\n",
    "del cwr_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Techniques\n",
    "\n",
    "We use this notebook to explore different sampling strategies, namely we investigate\n",
    "* `AgglomerativeClustering` to get a more diverse set, based on CWR vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "We compute the `Agglomerative Cluster` to understand our data, and strategicially take samples. In particular:\n",
    "\n",
    "1. Take the average `ELMo` vector to get a sentence embedding\n",
    "2. Build index of sentence embeddings for each instance in the set\n",
    "3. Build Agglomerative Cluster for dataset\n",
    "4. Support API to sample N points that should be the N most distinct points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedder\n",
    "\n",
    "We use this to define a sentence embedding, for now, we use `ELMo` vectors for average embedding. But since we build on the CWR abstraction this could easily be replaced with `BERT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cwr: CachedTextFieldEmbedder,\n",
    "        mode: str = 'avg',\n",
    "    ):\n",
    "        super(SentenceEmbedder, self).__init__()\n",
    "        self.cwr = cwr\n",
    "        self.mode = mode\n",
    "    \n",
    "    def _average_embedding(self, sentence_id: int, dataset_id: int) -> torch.Tensor:\n",
    "        cwr_embedding: torch.Tensor = self.cwr.get_embedding(\n",
    "            sentence_id=sentence_id,\n",
    "            dataset_id=dataset_id,\n",
    "        )\n",
    "        \n",
    "        return cwr_embedding.sum(dim=0) / len(cwr_embedding)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sentence_ids: torch.Tensor, # (batch_size, )\n",
    "        dataset_ids: torch.Tensor, # (batch_size, )\n",
    "    ) -> torch.Tensor:\n",
    "        embeddings: List[torch.Tensor] = [\n",
    "            self._average_embedding(\n",
    "                sentence_id=sentence_id.item(),\n",
    "                dataset_id=dataset_id.item(),\n",
    "            ).unsqueeze(0)\n",
    "            for sentence_id, dataset_id in zip(sentence_ids, dataset_ids)\n",
    "        ]\n",
    "        return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedder = SentenceEmbedder(cwr_elmo)\n",
    "# sentence_embedder(torch.LongTensor([0, 1]), dataset_ids=torch.LongTensor([0, 0])).shape\n",
    "# torch.Size([2, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the entire dataset\n",
    "\n",
    "We use this dataset to load in our training and validation data for use throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96867it [00:00, 345675.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_file, valid_file, test_file = get_dataset_files(dataset='CADEC')\n",
    "train_bio = BIODataset(\n",
    "    dataset_id=0,\n",
    "    file_name=train_file,\n",
    "    binary_class='ADR',\n",
    ")\n",
    "\n",
    "train_bio.parse_file()\n",
    "\n",
    "unlabeled_corpus = UnlabeledBIODataset(\n",
    "    dataset_id=train_bio.dataset_id,\n",
    "    bio_data=train_bio,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the Dataset\n",
    "\n",
    "Now that we have defined our sentence embedder, we can build an index of the entire dataset to apply our clustering algorithm on top of it\n",
    "\n",
    "```\n",
    "dataset_embed := TensorList()\n",
    "for inst in dataset:\n",
    "    sentence_embedding := embed(inst)\n",
    "    dataset_embed.add(sentence_embedding)\n",
    "return dataset_embed.numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 559.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_index(dataset: UnlabeledBIODataset, sentence_embedder) -> TensorList:\n",
    "    index = TensorList()\n",
    "    for inst in tqdm(dataset):\n",
    "        sentence_embedding: torch.Tensor = sentence_embedder(\n",
    "            sentence_ids=torch.Tensor([inst['id']]),\n",
    "            dataset_ids=torch.Tensor([dataset.dataset_id]),\n",
    "        )\n",
    "        index.append(sentence_embedding)\n",
    "    return index\n",
    "\n",
    "# TensorList(torch.Size([1000, 1024]))\n",
    "index: TensorList = build_index(unlabeled_corpus, sentence_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Now that our preprocessing is done, we use this section to build our clusters, in particular `index` stores all our data. We use the implementation in `scikit learn` and `scipy` to cluster our data and build a sampler.\n",
    "\n",
    "1. Cluster our data\n",
    "2. Build sampling API from cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "To cluster our data we rely on `sklearn.cluster.AgglomerativeClustering` and `scipy.cluster.hierarchy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(index: TensorList) -> AgglomerativeClustering:\n",
    "    '''\n",
    "    Runs agglomerative clustering on the `index` TensorList passed in\n",
    "    '''\n",
    "    index_np: np.ndarray = index.numpy()\n",
    "    model = AgglomerativeClustering(linkage=\"average\", affinity=\"cosine\", n_clusters=500, compute_full_tree=True)\n",
    "    model.fit(index_np)\n",
    "    return model\n",
    "\n",
    "def get_graph(index, model: AgglomerativeClustering) -> Dict[int, Dict[str, int]]:\n",
    "    '''\n",
    "    Given the results from an Agglomerative Cluster, returns the definition of the dendrogram\n",
    "    over the clusters, to allow for the clusters to be built\n",
    "    '''\n",
    "    ii = itertools.count(len(index))\n",
    "    graph = {next(ii): {'left': x[0], 'right':x[1]} for x in model.children_}\n",
    "    return graph\n",
    "\n",
    "model = get_clusters(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeirchicalClusterGraph(object):\n",
    "    @classmethod\n",
    "    def find_root(cls, adjacency_list: Dict[int, Dict[str, int]]) -> int:\n",
    "        '''\n",
    "        Given an adjacency list, return the node id for the root node\n",
    "        '''\n",
    "        all_left_children = [data['left'] for _, data in adjacency_list.items()]\n",
    "        all_right_children = [data['right'] for _, data in adjacency_list.items()]\n",
    "        all_children = set(all_left_children + all_right_children)\n",
    "        all_nodes = adjacency_list.keys()\n",
    "        potential_roots = list(filter(lambda x: x not in all_children, all_nodes))\n",
    "        assert len(potential_roots) == 1\n",
    "        return potential_roots[0]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        index: TensorList,\n",
    "    ):\n",
    "        self.index: TensorList = index\n",
    "        self.model: AgglomerativeClustering = get_clusters(index=index)\n",
    "        self.adjacency_list: Dict[int, Dict[str, int]] = get_graph(index=index, model=self.model)\n",
    "        self.root: int = HeirchicalClusterGraph.find_root(self.adjacency_list)\n",
    "        \n",
    "        self.layers: Dict[int, Set[int]] = {}\n",
    "        self.height: Dict[int, Set[int]] = {}\n",
    "        self.populate_layers(self.root, None, 0)\n",
    "    \n",
    "    def get_cluster_index(self, node_id: int) -> List[int]:\n",
    "        if self.is_leaf(node_id):\n",
    "            return [node_id]\n",
    "        members = []\n",
    "\n",
    "        children: Dict[str, int] = self.adjacency_list[node_id]\n",
    "        members.extend(self.get_cluster_index(children['left']))\n",
    "        members.extend(self.get_cluster_index(children['right']))\n",
    "        return members\n",
    "    \n",
    "    def is_leaf(self, node_id: Optional[int]) -> bool:\n",
    "        if node_id == None:\n",
    "            return True\n",
    "        children = node_id in self.adjacency_list and (\n",
    "            ('left' in self.adjacency_list[node_id]) or\n",
    "            ('right' in self.adjacency_list[node_id])\n",
    "        )\n",
    "\n",
    "        if not children:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def populate_layers(self, node_id: int, parent_node: Optional[int], layer: int) -> int:\n",
    "        if layer not in self.layers:\n",
    "            self.layers[layer] = set()\n",
    "        self.layers[layer].add(node_id)\n",
    "        if node_id not in self.adjacency_list:\n",
    "            self.adjacency_list[node_id] = {}\n",
    "        self.adjacency_list[node_id]['parent'] = parent_node\n",
    "        if not self.is_leaf(node_id):\n",
    "            children = self.adjacency_list.get(node_id, None)\n",
    "            lh = self.populate_layers(children['left'], node_id, layer + 1)\n",
    "            rh = self.populate_layers(children['right'], node_id, layer + 1)\n",
    "            h = max(lh, rh) + 1\n",
    "        else:\n",
    "            h = 0\n",
    "        \n",
    "        if h not in self.height:\n",
    "            self.height[h] = set()\n",
    "        self.height[h].add(node_id)\n",
    "        \n",
    "        return h\n",
    "            \n",
    "    \n",
    "    def sample_points(sample_size: int) -> List[int]:\n",
    "        '''\n",
    "        Given a sample size, samples `sample_size` points\n",
    "        from the HeirchicalCluster to ensure diversity\n",
    "        \n",
    "        returns indexes of points from original index\n",
    "        '''\n",
    "        pass\n",
    "graph = HeirchicalClusterGraph(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 2), (3, 4), (4, 6), (5, 8), (6, 10), (7, 12), (8, 12), (9, 10), (10, 8), (11, 6), (12, 12), (13, 12), (14, 12), (15, 16), (16, 18), (17, 12), (18, 8), (19, 12), (20, 12), (21, 10), (22, 10), (23, 10), (24, 10), (25, 14), (26, 20), (27, 12), (28, 18), (29, 22), (30, 20), (31, 24), (32, 24), (33, 16), (34, 14), (35, 6), (36, 4), (37, 4), (38, 4), (39, 8), (40, 4), (41, 2), (42, 4), (43, 6), (44, 6), (45, 8), (46, 10), (47, 10), (48, 12), (49, 4), (50, 4), (51, 6), (52, 6), (53, 6), (54, 6), (55, 10), (56, 6), (57, 6), (58, 8), (59, 6), (60, 12), (61, 8), (62, 10), (63, 12), (64, 16), (65, 16), (66, 16), (67, 14), (68, 8), (69, 10), (70, 6), (71, 4), (72, 6), (73, 8), (74, 12), (75, 8), (76, 14), (77, 16), (78, 24), (79, 26), (80, 26), (81, 22), (82, 14), (83, 16), (84, 14), (85, 14), (86, 14), (87, 14), (88, 14), (89, 14), (90, 20), (91, 8), (92, 10), (93, 10), (94, 12), (95, 16), (96, 8), (97, 12), (98, 8), (99, 10), (100, 12), (101, 16), (102, 16), (103, 14), (104, 8), (105, 8), (106, 14), (107, 18), (108, 18), (109, 24), (110, 22), (111, 16), (112, 22), (113, 22), (114, 14), (115, 12), (116, 12), (117, 10), (118, 4), (119, 8), (120, 16), (121, 14), (122, 16), (123, 18), (124, 22), (125, 20), (126, 22), (127, 20), (128, 18), (129, 14), (130, 8), (131, 8), (132, 6), (133, 8), (134, 12), (135, 16), (136, 22), (137, 26), (138, 22), (139, 18), (140, 20), (141, 34), (142, 36), (143, 30), (144, 30), (145, 20), (146, 22), (147, 18), (148, 10), (149, 12), (150, 12), (151, 8), (152, 10), (153, 8), (154, 4), (155, 4), (156, 4), (157, 4)]\n",
      "{'left': 384, 'right': 1997, 'parent': None}\n"
     ]
    }
   ],
   "source": [
    "print(list(map(lambda h: (h[0], len(h[1])), graph.layers.items())))\n",
    "print(graph.adjacency_list[1998])\n",
    "# '''\n",
    "# (0, {1998}),\n",
    "# (1, {384, 1997}),\n",
    "# (2, {1892, 1996}),\n",
    "# (3, {469, 1454, 1994, 1995}),\n",
    "# (4, {386, 984, 1356, 1991, 1992, 1993}),\n",
    "# (5, {336, 421, 445, 1250, 1980, 1987, 1989, 1990}),\n",
    "# (6, {359, 402, 699, 748, 1155, 1972, 1977, 1981, 1983, 1988}),\n",
    "# (7, {65, 66, 634, 780, 946, 982, 1127, 1954, 1962, 1967, 1974, 1986}),\n",
    "# (8, {186, 193, 269, 391, 411, 425, 630, 1109, 1713, 1894, 1943, 1985}),\n",
    "# (9, {106, 174, 623, 860, 905, 983, 1024, 1870, 1928, 1984}),\n",
    "# (10, {97, 139, 191, 410, 502, 1016, 1976, 1982}),\n",
    "# (11, {1009, 1015, 1965, 1968, 1971, 1979}),\n",
    "# (12, {263, 455, 685, 719, 746, 809, 1008, 1012, 1925, 1940, 1975, 1978}),\n",
    "# (13, {62, 158, 232, 662, 815, 839, 1007, 1011, 1961, 1964, 1970, 1973}),\n",
    "# (14, {39, 246, 488, 615, 1006, 1010, 1939, 1942, 1945, 1951, 1966, 1969})\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 256),\n",
       " (14, 26),\n",
       " (118, 20),\n",
       " (46, 17),\n",
       " (1, 14),\n",
       " (9, 13),\n",
       " (30, 9),\n",
       " (28, 8),\n",
       " (5, 8),\n",
       " (230, 7)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(model.labels_).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "Agglomerative Clustering has its problems, so lets try a more traditional approach KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_clusters(index: TensorList) -> KMeans:\n",
    "    '''\n",
    "    Runs agglomerative clustering on the `index` TensorList passed in\n",
    "    '''\n",
    "    index_np: np.ndarray = index.numpy()\n",
    "    model = KMeans(n_clusters=500, random_state=0)\n",
    "    model.fit(index_np)\n",
    "    return model\n",
    "model = get_kmeans_clusters(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(145, 39),\n",
       " (40, 26),\n",
       " (184, 24),\n",
       " (125, 21),\n",
       " (33, 20),\n",
       " (49, 20),\n",
       " (200, 19),\n",
       " (491, 18),\n",
       " (112, 17),\n",
       " (53, 17)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(model.labels_).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_snorkel)",
   "language": "python",
   "name": "conda_snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
